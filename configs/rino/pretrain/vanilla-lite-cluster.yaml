# Path placeholders:
# - PROJECT_ROOT: root directory of the project
# - EPOCHNUM: epoch number
# - JOBNAME: specified by name

name: dino-vanilla-lite-cluster

# Device configuration
device: "cuda"  # or "cpu"
accelerate: true  # whether to use Huggingface accelerate

# Parameters for the backbone model
model_type: "JetTransformerEncoder"
compile_model: false
model_params:
  jet_dim: 4    # dimension of the jet features
  proj_dim: 32  # representation dimension
  d_model: 64  # encoder's dimension
  nhead: 4  # number of heads in the multiheadattention models
  num_encoder_layers: 4  # number of sub-encoder-layers in the encoder
  dim_feedforward: 256  # dimension of the feedforward network model
  activation: "gelu"
  norm_first: true
  pooling: "cls_jet"  # aggregation method; class: use a class token; mean: use mean pooling
  d_head_hidden: 256  # hidden dimension of output head
  head_l2_norm: true   # whether to apply L2 normalization to the projection head's output before the last layer
  head_weight_norm: true  # whether to apply weight normalization to the last layer of the projection head

# Parameters for DINO Loss
loss_params:
  dino:
    # Don't specify output_dim and num_local_views; they are automatically determined
    warmup_teacher_temp: 0.04
    warmup_scheduler: cosine
    teacher_temp: 0.07
    warmup_teacher_temp_epochs: 30
    student_temp: 0.20
    koleo_loss_weight: 1.0
    enable_sinkhorn_knopp: true

pre_aug_norm: false  # whether to normalize the input before augmentation

# Augmentation parameters for different views
augmentation_params:
  augmenter_kwargs:
    need_part_p4: false
    need_jet_p4: false
  global:
    - name: "global_cluster"
      num: 4
      components:
        - type: "cluster"
          args:
            # choices to sample from
            # make sure num >= len(nprongs_choices)
            nprongs_choices: [1, 2, 3, 4]

  local:  # List of local views (stronger augmentations)
    - name: "local_cluster"
      num: 5
      components:
        - type: "cluster"
          args:  
            # choices to sample from
            # make sure num >= len(nprongs_choices)
            nprongs_choices: [8, 16, 32, 64, "ALL"]
  # local:  # List of local views (stronger augmentations)
  #   - name: "local_cluster"
  #     num: 4
  #     components:
  #       - type: "cluster"
  #         args:  
  #           # choices to sample from
  #           # make sure num >= len(nprongs_choices)
  #           nprongs_choices: [8, 16, 32, "ALL"]

shuffle:
  training: true
  inference: false

# Training parameters
training:
  dataloader:
    train:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/processed/ParT-QCD-shuffled42_cluster-kinematics.yaml"
      num_workers: 16 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 1000
        patterns: 
          - '**/*.h5'
        cache_size: 180
        size_multiplier: 2.5
        preload_workers: 10
    val:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/processed/ParT-QCD-shuffled42_cluster-kinematics.yaml"
      num_workers: 16 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 1000
        patterns: 
          - '**/*.h5'  # 10-19 (1% of the dataset; 500k jets) 
        cache_size: 20
        size_multiplier: 2.5
        preload_workers: 10
  num_epochs: 100
  patience: 1000
  grad_clip: 3.0 # gradient clip (if needed)
  autograd_detect_anomaly: false
  checkpoints_filename: "model_checkpoint_EPOCHNUM.pt"
  checkpoints_dir: "PROJECT_ROOT/experiments/JOBNAME/checkpoints"
  load_epoch: # epoch to reload
  teacher_momentum: 0.992  # EMA rate for teacher network update
  optimizer:
    name: "AdamW"  # DINO typically uses AdamW
    params:
      lr: 1.e-4  # 1e-4 for bs=1000
      betas: [0.9, 0.999]
      eps: 1.e-6
      weight_decay: 0.01
  scheduler:
    name: "SequentialLR"
    params:
      milestones: [5, 10]
      schedulers:
        - name: "CosineScheduler"
          params:
            base_value: 1.e-8
            final_value: 1.e-4
            total_iters: 5
        - name: "CosineScheduler"
          params:
            base_value: 1.e-4
            final_value: 1.0
            total_iters: 5
        - name: "CosineScheduler"
          params:
            base_value: 1.0
            final_value: 1.e-2
            total_iters: 90

# Inference parameters
inference:
  dataloader:
    # jetclass splits
    train_jetclass:
      preprocessed: false
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/raw/ParT-test-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 1000
        batch_size_atomic: 1000
        patterns: 
          - '**/T*.root'  # Top
          - '**/ZJetsToNuNu_*.root'  # QCD
        cache_size: 160
        size_multiplier: 2.5
        preload_workers: 6
    val_jetclass:
      preprocessed: false
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/raw/ParT-test-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 1000
        batch_size_atomic: 1000
        patterns: 
          - '**/T*.root'  # Top
          - '**/ZJetsToNuNu_*.root'  # QCD
        cache_size: 20
        size_multiplier: 2.5
        preload_workers: 6
    test_jetclass:
      preprocessed: false
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/raw/ParT-test-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 1000
        batch_size_atomic: 1000
        patterns: 
          - '**/T*.root'  # Top
          - '**/ZJetsToNuNu_*.root'  # QCD
        cache_size: 20
        size_multiplier: 2.5
        preload_workers: 6
    # jetnet splits
    train_jetnet:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/jetnet/ParT-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 1000
        patterns: 
          - 'gqt_train.h5'
        cache_size: 60
        size_multiplier: 2.5
        preload_workers: 10
    val_jetnet:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/jetnet/ParT-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 2000
        patterns: 
          - 'gqt_val.h5'
        cache_size: 20
        size_multiplier: 2.5
        preload_workers: 10
    test_jetnet:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/jetnet/ParT-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 2000
        patterns: 
          - 'gqt_test.h5'
        cache_size: 20
        size_multiplier: 2.5
        preload_workers: 10
    # jetnet150 splits
    train_jetnet150:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/jetnet150/ParT-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 1000
        patterns: 
          - 'gqt_train.h5'
        cache_size: 60
        size_multiplier: 2.5
        preload_workers: 10
    val_jetnet150:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/jetnet150/ParT-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 2000
        patterns: 
          - 'gqt_val.h5'
        cache_size: 20
        size_multiplier: 2.5
        preload_workers: 10
    test_jetnet150:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/jetnet150/ParT-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 2000
        patterns: 
          - 'gqt_test.h5'
        cache_size: 20
        size_multiplier: 2.5
        preload_workers: 10
    # Toptagging splits
    train_toptagging:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/toptagging/ParT-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 1000
        patterns: 
          - 'train.h5'
        cache_size: 60
        size_multiplier: 2.5
        preload_workers: 10
    val_toptagging:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/toptagging/ParT-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 2000
        patterns: 
          - 'validation.h5'
        cache_size: 20
        size_multiplier: 2.5
        preload_workers: 10
    test_toptagging:
      preprocessed: true
      cached: true
      config: "PROJECT_ROOT/configs/dataloaders/toptagging/ParT-kinematics.yaml"
      num_workers: 6 # number of workers for dataloader
      prefetch_factor: 8
      kwargs:
        batch_size: 2000
        patterns: 
          - 'validation.h5'
        cache_size: 20
        size_multiplier: 2.5
        preload_workers: 10
  # load_epoch: best
  load_epoch: [best, last]
  # splits: ["val_toptagging", "val_jetclass", "test_toptagging", "test_jetclass", "train_toptagging", "train_jetclass"]
  splits: ["test_jetnet", "test_jetnet150", "test_jetclass", "test_toptagging"]
  output_filename: "output_SPLIT_EPOCHNUM.pt"
  output_dir: "PROJECT_ROOT/experiments/JOBNAME/inference"